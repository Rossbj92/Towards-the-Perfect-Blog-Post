{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "There's 2 parts to this notebook. First, the Towardsdatascience archives are scraped to retrieve post URLs. Then, the actual posts are scraped from this list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For date ranges\n",
    "import calendar\n",
    "\n",
    "#Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "#Web scraping\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chromedriver = \"/Applications/chromedriver\" \n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "chrome_options = Options()\n",
    "#Headless so that you aren't driven mad with a new window every 7 seconds\n",
    "chrome_options.add_argument(\"--headless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_tuples(year, min_month, max_month):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns a (year, month, day) list of tuples.\n",
    "    \n",
    "    Function default is to break at the date 7/6. Modify\n",
    "    as necessary to get a wider/narrower range.\n",
    "    \"\"\"\n",
    "    months = range(min_month, max_month+1)\n",
    "    \n",
    "    date_range = []\n",
    "    \n",
    "    for m in months:\n",
    "\n",
    "        days = calendar.monthrange(2020,m)[1]\n",
    "\n",
    "        for d in range(1,days):\n",
    "            #Stopping at 7/6 - remove/change for custom stop date\n",
    "            if m == 7 and d > 6:\n",
    "                break\n",
    "            elif d < 10:\n",
    "                date_range.append((f'{year}', f'0{m}', f'0{d}'))\n",
    "            else:\n",
    "                date_range.append((f'{year}', f'0{m}', str(d)))\n",
    "    \n",
    "    return date_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_days_links(year, month, day):\n",
    "    \n",
    "    if day == 1:\n",
    "        daily_archive = f'https://towardsdatascience.com/archive/{year}/{month}'\n",
    "    else:\n",
    "        daily_archive = f'https://towardsdatascience.com/archive/{year}/{month}/{day}'\n",
    "    \n",
    "    response = requests.get(daily_archive).text\n",
    "    soup = BeautifulSoup(response)\n",
    "    \n",
    "    link_lst = []  \n",
    "    for a in soup.find_all('a', class_ = \"\", href = True):\n",
    "        link_lst.append(str(a))\n",
    "        \n",
    "    split_by_dashes = [i.split('---------') for i in link_lst if '---------' in i]\n",
    "\n",
    "    links = [i[3].split('=')[1].split('?')[0].strip('\"') for i in split_by_dashes]   \n",
    "    links_with_date = [[link, f'{month}/{day}/{year}'] for link in links]\n",
    "    \n",
    "    return links_with_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = date_tuples(2020, 1, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2020', '01', '01'), ('2020', '01', '02'), ('2020', '01', '03'), ('2020', '01', '04'), ('2020', '01', '05')]\n",
      "[('2020', '07', '02'), ('2020', '07', '03'), ('2020', '07', '04'), ('2020', '07', '05'), ('2020', '07', '06')]\n"
     ]
    }
   ],
   "source": [
    "print(date_range[:5])\n",
    "print(date_range[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_article_list = []\n",
    "\n",
    "for year,month,day in date_range:\n",
    "    full_article_list.append(retrieve_days_links(year, month, day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://towardsdatascience.com/making-python-programs-blazingly-fast-c1cd79bd1b32', '01/01/2020']\n",
      "['https://towardsdatascience.com/ten-python-development-skills-998a52f8f7c0', '07/06/2020']\n"
     ]
    }
   ],
   "source": [
    "#Sanity check for date range\n",
    "print(full_article_list[:1][0][0])\n",
    "print(full_article_list[-1:][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_page(link):\n",
    "    \"\"\"\n",
    "    \n",
    "    Retrieves an article with Selenium and outputs a Beautifulsoup object.\n",
    "    \n",
    "    Dynamic content may not load with near instantaneous opening and closing articles. \n",
    "    Thus, the page is allowed to load for 1 second. Additionally, some \n",
    "    elements (e.g., videos) did not render until they were viewed. Since these \n",
    "    elements are always nested in 'figure' tags, the driver locates and subsequently\n",
    "    scrolls to each one. Following this, 1 more second is allowed to pass before\n",
    "    the page is turned into soup.\n",
    "\n",
    "    \"\"\"\n",
    "    #Activate headless driver \n",
    "    driver = webdriver.Chrome(chromedriver, options=chrome_options)\n",
    "    driver.get(link)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    #Locate + scroll to figures\n",
    "    #Code adapted from https://stackoverflow.com/questions/48006078/how-to-scroll-down-in-python-selenium-step-by-step\n",
    "    read_mores = driver.find_elements_by_tag_name('figure')\n",
    "    for read_more in read_mores:\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", read_more)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html5lib')\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_claps(soup):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns number of claps article received.\n",
    "    \n",
    "    This searches the article for the claps, which \n",
    "    are located in a button at the end of the article.\n",
    "    The try/except block is present due to an error\n",
    "    ocurring if no claps are found.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        for button in soup.find_all('button'):\n",
    "            if 'claps' in str(button):\n",
    "                clap_button = str(button).split('claps')\n",
    "                claps = clap_button[0].split('>')[1]\n",
    "    \n",
    "        return claps\n",
    "    \n",
    "    except:\n",
    "        return 0\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sections(soup):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns number of section in article.\n",
    "    \n",
    "    This searches for all h1 and h2 tags, with \n",
    "    a section defined as the presence of either.\n",
    "    The minimum value that is returned is 1, since\n",
    "    every article will have an h1 tag for the title.\n",
    "    \n",
    "    \"\"\"\n",
    "    h1 = []\n",
    "    for main in soup.find_all('h1'):\n",
    "        h1.append(main)\n",
    "\n",
    "    h2 = []\n",
    "    for sub in soup.find_all('h2'):\n",
    "        h2.append(sub)\n",
    "\n",
    "    return len(h1) + len(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_codeblocks(soup):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns both static and interactive codeblocks.\n",
    "    \n",
    "    Codeblocks on Medium can either be fixed and inside\n",
    "    a 'pre' tag, or they can be dynamic. These dynamic\n",
    "    blocks are nested in iframes. Videos are as well, \n",
    "    but codeblocks can be differentiated by their lack\n",
    "    of 'YouTube' in the source. The main limitation is \n",
    "    videos not embedded from YouTube will be counted as\n",
    "    codeblocks, but I have yet to find any non-YouTube\n",
    "    videos.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    static_count = 0\n",
    "    for codeblock in soup.find_all('pre'):\n",
    "        static_count += 1\n",
    "\n",
    "    dynamic_count = 0\n",
    "    for codeblock in soup.find_all('figure'):\n",
    "        for frame in codeblock.find_all('iframe'):\n",
    "            if 'YouTube' not in str(frame):\n",
    "                dynamic_count += 1\n",
    "    \n",
    "    return static_count + dynamic_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_videos(soup):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns number of videos in article.\n",
    "    \n",
    "    All videos are located in iframe tags. \n",
    "    Currently, this function only locates\n",
    "    videos embedded from YouTube.\n",
    "    \n",
    "    \"\"\"\n",
    "    videos = 0\n",
    "    \n",
    "    for fig in soup.find_all('figure'):\n",
    "        for frame in fig.find_all('iframe'):\n",
    "            if 'YouTube' in str(frame):\n",
    "                videos += 1\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(link):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    This function parses a Medium article URL page into a Pandas DataFrame.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    soup = load_page(link[0])\n",
    "    \n",
    "    article = soup.find('article')\n",
    "    \n",
    "    article_df = pd.DataFrame(index = [0], columns = ['date', 'title', 'post', 'num_sections', \n",
    "                                                      'num_images', 'num_codeblocks'])\n",
    "    \n",
    "    #Date\n",
    "    article_df['date'] = pd.to_datetime(link[1])\n",
    "    \n",
    "    #Actual article\n",
    "    article_df['post'] = (' ').join([p.text for p in article.find_all('p')])\n",
    "    \n",
    "    #Main sections\n",
    "    headers = [header.text for header in article.find_all('h1')]\n",
    "    article_df['title'] = headers[0]\n",
    "    article_df['num_sections'] = find_sections(article)\n",
    "    \n",
    "    #Code blocks\n",
    "    article_df['num_codeblocks'] = find_codeblocks(article)\n",
    "    \n",
    "    #Images\n",
    "    article_df['num_images'] = len([img for img in article.find_all('figure', class_='paragraph-image')])\n",
    "    \n",
    "    #Videos\n",
    "    article_df['num_videos'] = find_videos(article)\n",
    "    \n",
    "    #Claps\n",
    "    article_df['claps'] = find_claps(soup)\n",
    "\n",
    "    \n",
    "    return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_posts(days):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns a dataframe consisting of scraped posts.\n",
    "    \n",
    "    The function references 'full_article_list' and will \n",
    "    return all posts for however many days (i.e., indices)\n",
    "    are entered. Posts are scraped using the 'parse_article'\n",
    "    function above.\n",
    "    \n",
    "    \"\"\"\n",
    "    post_df = pd.DataFrame()\n",
    "\n",
    "    days = range(days+1)\n",
    "    \n",
    "    for day in days:\n",
    "        for article in full_article_list[day]:\n",
    "            try:\n",
    "                post_df = post_df.append(parse_article(article))\n",
    "            #articles from TDSTeam have no explicit title and threw IndexErrors. Decided to just exclude since they were\n",
    "            #links to other articles with descriptions, rather than an article in itself.\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(full_article_list)} days of articles.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = scrape_posts(188)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posts.to_pickle('./Data/medium_2020_posts.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
